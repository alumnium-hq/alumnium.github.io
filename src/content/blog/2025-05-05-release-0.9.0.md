---
title: "Alumnium Release 0.9.0"
pubDate: 2025-05-05
description: "Alumnium supoorts Mistral Small 3.1 24B and Ollama since v0.9.0 release!"
author:
  name: "Alex Rodionov"
  image: alex_rodionov.jpeg
tags: ["release notes"]
---

Itâ€™s been a while since our last release due to vacations, but weâ€™re now catching up and [v0.9.0](https://github.com/alumnium-hq/alumnium/releases/tag/0.9.0) was published yesterday.

## What's changed?

The biggest highlight is support for a completely **local model** inference using Mistral Small 3.1 24B and Ollama. This means that if you have a decent hardware (e.g. RTX 4090), you can run Alumnium fully on your machine, without the need for cloud-based AI providers! Check out [the docs](https://alumnium.ai/docs/getting-started/configuration/#ollama)! ðŸ¦™

Apart from that, weâ€™ve done a lot of work to make the project more welcoming for new contributors, improved CI and gardened a code for the upcoming features - improving stability and performance with caching LLM responses, exploring Appium support and others. Stay tuned and join our [Discord](https://discord.gg/mP29tTtKHg)!
